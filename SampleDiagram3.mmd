flowchart LR
  %% ===== GROUP: Step 1 - Load models =====
  subgraph G1[Step 1 - Load models]
	A1[UnetLoaderGGUF<br/>Qwen-Image-Edit-2509 GGUF]
	A2[LoraLoaderModelOnly<br/>Qwen-Image-Lightning LoRA]
	A3[ModelSamplingAuraFlow]
	A4[CFGNorm]
	A1 --> A2 --> A3 --> A4
  end

  subgraph G1b[Text Encoder & VAE]
	B1[CLIPLoader<br/>qwen_2.5_vl_7b_fp8_scaled]
	B2[VAELoader<br/>qwen_image_vae]
  end

  %% ===== GROUP: Step 2 - Upload image(s) =====
  subgraph G2[Step 2 - Upload image for editing]
	C1[LoadImage #1<br/>dark portrait]
	C2[LoadImage #2<br/>optional ref]
	C3[LoadImage #3<br/>optional ref]
  end

  %% ===== GROUP: Step 3 - Size control / Latent prep =====
  subgraph G3[Step 3 - Image Size / Latent]
	D1[ImageScaleToTotalPixels<br/>avoid oversized inputs]
	D2[VAEEncode<br/>encode scaled image to latent]
	D3[EmptySD3LatentImage<br/>optional: replace VAEEncode]
  end

  %% ===== GROUP: Step 4 - Prompting =====
  subgraph G4[Step 4 - Prompt / Conditioning]
	E1[TextEncodeQwenImageEditPlus<br/>POSITIVE prompt + images]
	E2[TextEncodeQwenImageEditPlus<br/>ALT prompt branch]
  end

  %% ===== GROUP: Sampler / Decode / Save =====
  subgraph G5[Sampling to Decode to Save]
	F1[KSampler<br/>steps / cfg / sampler / scheduler / denoise]
	F2[VAEDecode<br/>latent to image]
	F3[SaveImage]
  end

  %% ===== Connections across groups =====
  %% Encoders & VAE
  B1 --> E1
  B2 --> E1
  B1 --> E2
  B2 --> E2

  %% Images into scaler
  C1 --> D1
  C2 --> E1
  C2 --> E2
  C3 --> E1
  C3 --> E2
  D1 --> D2

  %% Scaled image also into Text Encoder (image1)
  D1 --> E1
  D1 --> E2

  %% Model path into sampler
  A4 --> F1

  %% Conditioning into sampler
  E1 -- positive/negative --> F1
  E2 -- (alternative prompt branch) --> F1

  %% Latent into sampler (choose one)
  D2 --> F1
  D3 -. optional .-> F1

  %% Decode & save
  F1 --> F2 --> F3
